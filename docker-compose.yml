services:
  # 0) PostgreSQL (internal)
  db:
    image: postgres:16
    container_name: sempervigil-db
    environment:
      POSTGRES_DB: ${SV_DB_NAME:-sempervigil}
      POSTGRES_USER: ${SV_DB_USER:-sempervigil}
      POSTGRES_PASSWORD: ${SV_DB_PASSWORD:-sempervigil}
    volumes:
      - sempervigil_pgdata:/var/lib/postgresql/data
    networks:
      - svnet
    restart: unless-stopped

  # 1) Admin API (internal)
  admin:
    build:
      context: .
      dockerfile: docker/ingest/Dockerfile
    container_name: sempervigil-admin
    env_file:
      - .env
    environment:
      SV_ENV: ${SV_ENV:-dev}
      SV_DATA_DIR: /data
      SV_LOG_LEVEL: ${SV_LOG_LEVEL:-INFO}
      SV_LOG_FILE: /data/logs/admin.log
      SV_UMASK: ${SV_UMASK:-002}
      SV_ADMIN_TOKEN: ${SV_ADMIN_TOKEN:-}
      SV_DB_URL: ${SV_DB_URL:-}
    command:
      - sh
      - -lc
      - |
        set -e
        umask ${SV_UMASK:-002}
        sh /tools/ensure-dirs.sh
        uvicorn sempervigil.admin:app --host 0.0.0.0 --port 8000
    ports:
      - "0.0.0.0:${SV_ADMIN_PORT:-8001}:8000"
    user: "${SV_UID:-1000}:${SV_GID:-1000}"
    volumes:
      - sv_data:/data
      - ./tools:/tools:ro
    networks:
      - svnet
    restart: unless-stopped

    # 2) Worker (internal)
  # Recommended scaling: worker_fetch=2, worker_llm=1
  worker_fetch:
    build:
      context: .
      dockerfile: docker/ingest/Dockerfile
    # NOTE: no container_name so we can scale this service
    hostname: worker-fetch
    labels:
      com.sempervigil.role: "worker"
      com.sempervigil.queue: "fetch"
      com.sempervigil.service: "worker_fetch"
    env_file:
      - .env
    environment:
      SV_ENV: ${SV_ENV:-dev}
      SV_DATA_DIR: /data
      SV_LOG_LEVEL: ${SV_LOG_LEVEL:-INFO}
      SV_LOG_FILE: /data/logs/worker.log
      SV_UMASK: ${SV_UMASK:-002}
      SV_DB_URL: ${SV_DB_URL:-}
      SV_WORKER_ONLY_TYPES: "ingest_due_sources,ingest_source,html_index,rss_index,fetch_article_content,write_article_markdown,cve_sync,derive_events_from_articles"
    command:
      - sh
      - -lc
      - |
        set -e
        umask ${SV_UMASK:-002}
        sh /tools/ensure-dirs.sh
        sempervigil-worker
    user: "${SV_UID:-1000}:${SV_GID:-1000}"
    volumes:
      - sv_data:/data
      - sv_site:/site
      - ./tools:/tools:ro
    networks:
      - svnet
    restart: unless-stopped

  worker_llm:
    build:
      context: .
      dockerfile: docker/ingest/Dockerfile
    # NOTE: no container_name so we can scale this service
    hostname: worker-llm
    labels:
      com.sempervigil.role: "worker"
      com.sempervigil.queue: "llm"
      com.sempervigil.service: "worker_llm"
    env_file:
      - .env
    environment:
      SV_ENV: ${SV_ENV:-dev}
      SV_DATA_DIR: /data
      SV_LOG_LEVEL: ${SV_LOG_LEVEL:-INFO}
      SV_LOG_FILE: /data/logs/worker.log
      SV_UMASK: ${SV_UMASK:-002}
      SV_DB_URL: ${SV_DB_URL:-}
      SV_WORKER_ONLY_TYPES: "summarize_article_llm"
      SV_LLM_MAX_INFLIGHT: "1"
      SV_WORKER_CONCURRENCY: "1"
    command:
      - sh
      - -lc
      - |
        set -e
        umask ${SV_UMASK:-002}
        sh /tools/ensure-dirs.sh
        sempervigil-worker
    user: "${SV_UID:-1000}:${SV_GID:-1000}"
    volumes:
      - sv_data:/data
      - sv_site:/site
      - ./tools:/tools:ro
    networks:
      - svnet
    restart: unless-stopped

    # 3) Builder (on-demand Hugo build)
  builder:
    profiles:
      - build
    build:
      context: .
      dockerfile: docker/builder/Dockerfile
    env_file:
      - .env
    environment:
      SV_UMASK: ${SV_UMASK:-002}
      SV_LOG_FILE: /data/logs/builder.log
      SV_HUGO_BASEURL: ${SV_HUGO_BASEURL:-http://localhost:${SV_WEB_PORT:-8080}/}
      HUGO_CACHEDIR: /tmp/hugo_cache
      SV_DB_URL: ${SV_DB_URL:-}

      # IMPORTANT: build from a writable copy, not the read-only bind mount
      SV_HUGO_SOURCE_DIR: /tmp/site-src
      SV_HUGO_OUTPUT_DIR: /site

      # (Optional but helpful if your builder code supports it)
      SV_HUGO_TMP_SOURCE_DIR: /tmp/site-src
    working_dir: /repo
    user: "${SV_UID:-1000}:${SV_GID:-1000}"
    volumes:
      - .:/repo:ro
      - sv_data:/data
      - sv_site:/site

      # caches (safe to write)
      - sv_hugo_cache:/tmp/hugo_cache
      - sv_hugo_modules:/tmp/hugo_modules

      # If your theme uses Hugo image pipeline and expects persistence, keep this.
      # Otherwise, you can REMOVE this line entirely (recommended for portability),
      # because /tmp/site-src will have its own writable resources/ each run.
      # - sv_hugo_resources:/tmp/site-src/resources

      - ./tools:/tools:ro

    # Copy Hugo source into a writable temp dir, then run the builder once.
    # (Doesn't require any code changes in the python builder.)
    command:
      - sh
      - -lc
      - |
        set -e
        umask ${SV_UMASK:-002}
        sh /tools/ensure-dirs.sh

        rm -rf /tmp/site-src
        mkdir -p /tmp/site-src
        cp -a /repo/site/. /tmp/site-src/

        # If your site expects a "resources" dir, ensure it exists
        mkdir -p /tmp/site-src/resources

        # Run the builder once (it will run Hugo using SV_HUGO_SOURCE_DIR/SV_HUGO_OUTPUT_DIR)
        sempervigil-builder --once
    networks:
      - svnet
    restart: "no"

  # 4) Static site server (public)
  web:
    image: nginxinc/nginx-unprivileged:alpine
    container_name: sempervigil-web
    ports:
      - "${SV_WEB_PORT:-8080}:8080"
    volumes:
      - sv_site:/usr/share/nginx/html:ro
    networks:
      - svnet
    restart: unless-stopped

  # 5) Test runner (image-only, no NFS)
  test:
    profiles:
      - test
    build:
      context: .
      dockerfile: docker/ingest/Dockerfile
      args:
        INSTALL_TEST_DEPS: "1"
    env_file:
      - .env
    user: "${SV_UID:-1000}:${SV_GID:-1000}"
    volumes:
      - sv_data:/data
      - sv_site:/site
    working_dir: /app
    command: ["python", "-m", "pytest", "-q", "-p", "no:cacheprovider"]
    environment:
      SV_DB_URL: ${SV_DB_URL:-}

volumes:
  sv_data:
  sv_site:
  sv_hugo_cache:
  sv_hugo_resources:
  sv_hugo_modules:
  sempervigil_pgdata:

networks:
  svnet:
    driver: bridge
